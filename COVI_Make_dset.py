#!/usr/bin/env python2.7
import argparse, os, re, subprocess, itertools
from collections import defaultdict
from math import sqrt
import COVIclientmodules.FsFormats as FsF

def make_covi_dataset(aparc_fi, inflated_surface, matrix_fi, annot_1D_cmap,
    path):
    '''
    aparc_fi: the desired cortical parcellation file
    annot_1D_cmap: an .annot.1D.cmap file generated by SUMA_make_spec_*
    inflated_surface: An inflated FreeSurfer ASCII (.asc) surface
    matrix_fi: the 1D results file from @ROI_Corr_Mat
    path: The path to write out the dataset
    '''
    try:
        all_nodes_xyz = FsF.read_surface(inflated_surface.name)["nodes"]
        annot_node = defaultdict(list)
        annot_data, color_table = FsF.read_annot(aparc_fi.name)
        for node, annot in enumerate(annot_data):
            annot_node[annot].append(node)
        #roi_names = {annot:color_table[annot][0] for annot in color_table }

        cmap = FsF.read_1D_cmap(annot_1D_cmap.name)
        ilabel_annot = { cm[1]:cm[-3] for cm in cmap }
        ilabel_matrix_header, matrix = FsF.read_ROI_Corr_Matrix(matrix_fi.name)

        # Translate the ilabels the matrix is indexed with to annotations
        matrix = { ilabel_annot[ilabel]:matrix[ilabel] for ilabel in matrix }
        annot_matrix_header = [ilabel_annot[i] for i in ilabel_matrix_header]
        _filter = [annot in annot_node for annot in annot_matrix_header]
        print ilabel_matrix_header
        print annot_matrix_header
        print annot_node.keys()
        print _filter
        print [annot in annot_matrix_header for annot in annot_node.keys()]
        # Filter out non-cortical areas
        corr_matrix = {}
        '''
        for i in xrange(len(annot_matrix_header)):
            if _filter[i]:
                corr_matrix[annot_matrix_header[i]] = 
                
                
        
        '''
        for roi in annot_node:
            try:
                """
                corr_matrix[roi] = [corr for i, corr in enumerate(matrix[roi]) 
                    if _filter[i]]
                """
                corr_matrix[roi] = [n for n in 
                    itertools.compress(matrix[roi], _filter)]
            except KeyError:
                pass
        #annot_list = [annot for i, annot in enumerate(annot_matrix_header) 
        #            if _filter[i]]
        annot_list = [n for n in itertools.compress(annot_matrix_header, _filter)]
        #matrix = { roi:matrix[roi] for roi in annot_node }
        

    except (ValueError, IOError) as e:
        print e
        
    try:
        create_stat_files(corr_matrix, annot_list, path)
        create_clust_file(annot_node, all_nodes_xyz, path)
    except IOError as e:
        print "Could not write out dataset:"
        print e


def create_stat_files(corr_matrix, matrix_header, path):
    '''
    Create a .stat.1D file for each cluster.
    corr_matrix: The 1D correlation matrix generated by @ROI_Corr_Mat
    '''
    roi_list = corr_matrix.keys()
    roi_list.sort()
    for roi in roi_list:
        stat_fi = open(os.path.join(path,"%s.stat.1D"%(roi)), 'w')
        #stat_fi.writelines((str(i) for i in corr_matrix[roi]))
        #TODO: Handle negative values!
        [stat_fi.write("%i %s\n"%(roi_2, str(abs(i)))) 
            for roi_2, i in itertools.izip(matrix_header, corr_matrix[roi])]


def create_clust_file(roi_node_map, all_nodes_xyz, path, roi_names=None):
    '''
    Creates the cluster.1D file necessary for a COVI dataset.

    all_nodes_xyz: the "nodes" list from an inflated surface asc file
        parsed by FsFormats.read_surface
    '''
    #TODO: Add ROI names to cluster file
    clust_fi = open(os.path.join(path,"clusters.1D"), 'w')
    roi_list = roi_node_map.keys()
    roi_list.sort()
    for seq, roi in enumerate(roi_list):
        nodes = roi_node_map[roi]
        nodes_xyz = {}
        [nodes_xyz.__setitem__(node, all_nodes_xyz[node])
            for node in nodes]

        cent_x = sum([1./len(nodes)*i[0] for i in nodes_xyz.itervalues()])
        cent_y = sum([1./len(nodes)*i[1] for i in nodes_xyz.itervalues()])
        cent_z = sum([1./len(nodes)*i[2] for i in nodes_xyz.itervalues()])

        cent = [cent_x, cent_y, cent_z]

        min_dist = 10**5
        distance = lambda x,y: sqrt((x[0]-y[0])**2+(x[1]-y[1])**2+(x[2]-y[2])**2)
        for i in nodes_xyz:
            try:
                dist = distance(nodes_xyz[i], cent)
            except:
                print 'nodes_xyz[i]: ',
                print nodes_xyz[i]
                raise 
            if dist < min_dist:
                min_dist = dist
                cent_node = i

        clust_fi.write('%i\n'%(roi))
        if roi_names:
            clust_fi.write('%s\n'%(str(roi_names[seq])))
            
        clust_fi.write('%i\n'%(cent_node))
        del nodes_xyz[cent_node]
        for i in nodes_xyz:
            clust_fi.write('%i\n'%(i))
        clust_fi.write('\n')

def writeable_dir(op):
    if os.path.isdir(op) and os.access(op, os.W_OK):
        return op
    else:
        try:
            os.mkdir(op)
            return op
        except os.error:
            raise argparse.ArgumentError(
                "%s is not a writable directory, "%(str(op))+
                "and could not be created.")

def afni_head_brik(op):
    if (os.access(op+'.HEAD', os.R_OK) or os.path.isdir(op+'.head', os.R_OK) and
        os.access(op+'.BRIK', os.R_OK) or os.path.isdir(op+'.brik', os.R_OK)): 
        return op
    else:
        raise argparse.ArgumentError(
            "%s is missing a HEAD or BRIK file. "%(str(op)))

if __name__ == '__main__':
    # Parse arguments
    '''
    TODO:
    -Parse arguments
    -Parse the spec file
    -tgz the SurfVol, spec file, and attached surfaces
    Arguments:
    -op <output path>
    -spec <lh or rh spec>
    -sv <SurfVol>
    -matrix <1D covariance matrix>
    -annot1droi <.annot.1D.roi file>
    '''

    parser = argparse.ArgumentParser(
        description="COnnectome VIsualizer dataset creation script")
    """
    parser.add_argument('-spec', action='store', type=file,
        help="<left or right hemisphere spec file>", required=True)
    """
    parser.add_argument('-inf', '--inflated', action='store', type=file,
        help="<inflated surface file>", required=True)
    """
    parser.add_argument('-sv', '--SurfVol',  action='store', type=afni_head_brik,
        help="<SurfVol file>", required=True)
    """
    parser.add_argument('-mat', '--matrix', action='store', type=file,
        help="<a .corr.1D file output from @ROI_Corr_Mat>", required=True)
    parser.add_argument('-aparc', action='store', type=file,
        help="<an .aparc.a2009s.annot file, output by FreeSurfer>", required=True)
    parser.add_argument('-cmap', action='store', type=file,
        help="<the .annot.1D.cmap file, output by SUMA_make_spec_FS "+
        "for the left or right hemisphere>", required=True)
    """
    parser.add_argument('-roi', '--annot1droi', action='store', type=file,
        help="<the .annot.1D.roi file, output by SUMA_make_spec_FS "+
        "for the left or right hemisphere>", required=True)
    """
    parser.add_argument('-prefix', action='store', type=writeable_dir,
        help="<the prefix for the output files>", required=True)
    results = parser.parse_args()

    for i in results.__dict__:
        print i
        print getattr(results, i)

    make_covi_dataset(results.aparc, results.inflated, 
        results.matrix, results.cmap, results.prefix)
